{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lung Cancer Detection Using CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import libraries for data imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Read From Pickled Files (Instead of Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def pickle2np(dicts):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for data_dict in dicts:\n",
    "        images.append(data_dict['features'])\n",
    "        labels.append(data_dict['labels'])\n",
    "    return np.concatenate([img for img in images],axis=0), np.concatenate([lbl for lbl in labels], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_dicts = []\n",
    "for i in range(1,8):\n",
    "    path = '../downloads/train_set'+str(i)+'.p'\n",
    "    with open(path, mode='rb') as f:\n",
    "        train_dicts.append(pickle.load(f))\n",
    "train_images, train_labels = pickle2np(train_dicts)\n",
    "del train_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "valid_dicts = []\n",
    "for i in range(1,3):\n",
    "    path = '../downloads/valid_set'+str(i)+'.p'\n",
    "    with open(path, mode='rb') as f:\n",
    "        valid_dicts.append(pickle.load(f))\n",
    "valid_images, valid_labels = pickle2np(valid_dicts)\n",
    "del valid_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_dicts = []\n",
    "path = '../downloads/test.p'\n",
    "with open(path, 'rb') as f:\n",
    "    test_dicts.append(pickle.load(f))\n",
    "test_images, test_ids = pickle2np(test_dicts)\n",
    "del test_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "\n",
    "## Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def center_and_normalize(data, mu, dev):\n",
    "    return (data-mu)/dev \n",
    "\n",
    "process_mu = np.mean(train_images)\n",
    "process_dev = np.std(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, n_labels):\n",
    "    encoded_labels = np.zeros((labels.shape[0], n_labels), dtype=np.float32)\n",
    "    for i,label in enumerate(labels):\n",
    "        encoded_labels[i,int(label)] = 1\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(train_images.shape[0]):\n",
    "    train_images[i:i+1] = center_and_normalize(train_images[i:i+1],process_mu, process_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "valid_images,valid_labels = shuffle(valid_images,valid_labels)\n",
    "valid_images = valid_images[:100]\n",
    "valid_labels = valid_labels[:100]\n",
    "valid_images = center_and_normalize(valid_images, process_mu, process_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape([train_images.shape[i] for i in range(3)]+[1])\n",
    "valid_images = valid_images.reshape([valid_images.shape[i] for i in range(3)]+[1])\n",
    "train_labels = one_hot_encode(train_labels,2)\n",
    "valid_labels = one_hot_encode(valid_labels,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "## Tensorflow Approach\n",
    "\n",
    "#### Parameter Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "convweight_shapes = [\n",
    "    # 512x512\n",
    "    (5,5,1,36), # 255x255 stride=2\n",
    "    (5,5,36,26), # 126x126 stride=2\n",
    "    (4,4,26,50), # 61x61 stride=2\n",
    "    (61,61,50,2)\n",
    "]\n",
    "\n",
    "convweights = [tf.Variable(tf.truncated_normal(shape=x,mean=0,stddev=0.1),name=\"reg_conv\"+str(x[-1])) for x in convweight_shapes]\n",
    "convbiases = [tf.Variable(tf.zeros([x[-1]]),name=\"reg_convbias\"+str(x[-1])) for x in convweight_shapes]\n",
    "del convweight_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "#### Neural Net Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv2d(data, weight, bias, stride=1, padding=\"VALID\"):\n",
    "    activations = tf.nn.bias_add(tf.nn.conv2d(data, weight,strides=[1,stride,stride,1],padding=padding),bias)\n",
    "    return tf.nn.elu(activations)\n",
    "\n",
    "def max_pool(data,k=2):\n",
    "    return tf.nn.max_pool(data,ksize=[1,k,k,1],strides=[1,k,k,1],padding=\"VALID\")\n",
    "\n",
    "def conv_net(data, weights, biases, dropout_prob, strides=[]):\n",
    "    if len(strides) == 0: strides = [1]*len(weights)\n",
    "    logits = data\n",
    "    for i,weight in enumerate(weights):\n",
    "        logits = conv2d(logits, weight, biases[i],stride=strides[i])\n",
    "#         logits = max_pool(logits)\n",
    "        logits = tf.nn.dropout(logits, dropout_prob)\n",
    "    return logits\n",
    "\n",
    "def fc_net(data, weights, biases, dropout_prob):\n",
    "    logits = data\n",
    "    for i,weight in enumerate(weights):\n",
    "        if i < len(weights)-1:\n",
    "            logits = tf.matmul(logits, weight) + biases[i]\n",
    "            logits = tf.nn.elu(logits)\n",
    "    #         logits = tf.nn.dropout(logits, dropout_prob)\n",
    "    return tf.matmul(logits,weights[-1])+biases[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_features = tf.placeholder(tf.float32, [None]+[train_images.shape[i] for i in range(1,len(train_images.shape))], name=\"data_features\")\n",
    "data_labels = tf.placeholder(tf.float32, [None, 2], name='data_labels')\n",
    "\n",
    "convdropout = tf.placeholder(tf.float32, name=\"convdropout\")\n",
    "momentum = tf.placeholder(tf.float32, name=\"momentum\")\n",
    "learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Fully Convolutional Architecture\n",
    "\n",
    "logits = conv_net(data_features, convweights, convbiases, convdropout, strides=[2,2,2,1,1])\n",
    "logits = tf.reshape(logits, [-1,2])\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=data_labels))\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=momentum).minimize(cost)\n",
    "\n",
    "equals_list = tf.equal(tf.argmax(logits,1), tf.argmax(data_labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(equals_list,tf.float32))\n",
    "\n",
    "save_file = './net.ckpt'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Covolutional tf Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Start\n",
      "Epoch: 1\n",
      "Non-Cancer Percentage: 0.61\n",
      "Running Cost (Batch 10): 0.684477, Acc: 0.63\n",
      "Non-Cancer Percentage: 0.64\n",
      "Running Cost (Batch 20): 0.688064, Acc: 0.64\n",
      "Non-Cancer Percentage: 0.6\n",
      "Running Cost (Batch 30): 0.709639, Acc: 0.59\n",
      "Non-Cancer Percentage: 0.58\n",
      "Running Cost (Batch 40): 0.695588, Acc: 0.6\n",
      "Non-Cancer Percentage: 0.55\n",
      "Running Cost (Batch 50): 0.719604, Acc: 0.54\n",
      "\n",
      "Actual Cancer Percentage: 0.35\n",
      "Cost: 0.693147, Accuracy: 0.7\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "Non-Cancer Percentage: 0.51\n",
      "Running Cost (Batch 10): 0.709639, Acc: 0.51\n",
      "Non-Cancer Percentage: 0.62\n",
      "Running Cost (Batch 20): 0.719604, Acc: 0.6\n",
      "Non-Cancer Percentage: 0.56\n",
      "Running Cost (Batch 30): 0.698029, Acc: 0.58\n",
      "Non-Cancer Percentage: 0.64\n",
      "Running Cost (Batch 40): 0.712579, Acc: 0.64\n",
      "Non-Cancer Percentage: 0.54\n",
      "Running Cost (Batch 50): 0.711432, Acc: 0.56\n",
      "\n",
      "Actual Cancer Percentage: 0.35\n",
      "Cost: 0.693147, Accuracy: 0.58\n",
      "\n",
      "\n",
      "Epoch: 3\n",
      "Non-Cancer Percentage: 0.52\n",
      "Running Cost (Batch 10): 0.71781, Acc: 0.5\n",
      "Non-Cancer Percentage: 0.64\n",
      "Running Cost (Batch 20): 0.728423, Acc: 0.63\n",
      "Non-Cancer Percentage: 0.65\n",
      "Running Cost (Batch 30): 0.759963, Acc: 0.57\n",
      "Non-Cancer Percentage: 0.63\n",
      "Running Cost (Batch 40): 0.726629, Acc: 0.56\n",
      "Non-Cancer Percentage: 0.61\n",
      "Running Cost (Batch 50): 0.687417, Acc: 0.59\n",
      "\n",
      "Actual Cancer Percentage: 0.35\n",
      "Cost: 0.693147, Accuracy: 0.66\n",
      "\n",
      "\n",
      "Epoch: 4\n",
      "Non-Cancer Percentage: 0.66\n",
      "Running Cost (Batch 10): 0.711432, Acc: 0.7\n",
      "Non-Cancer Percentage: 0.51\n",
      "Running Cost (Batch 20): 0.745912, Acc: 0.48\n",
      "Non-Cancer Percentage: 0.58\n",
      "Running Cost (Batch 30): 0.74068, Acc: 0.56\n",
      "Non-Cancer Percentage: 0.56\n",
      "Running Cost (Batch 40): 0.754083, Acc: 0.52\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-300f570311c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                                                                        \u001b[0mconvdropout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconv_dropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                                                        \u001b[0mdata_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                                                        data_labels: train_labels[(batch-1)*batch_size:batch*batch_size]})\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Non-Cancer Percentage: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rate = .005\n",
    "moment = .75\n",
    "conv_dropout = .9\n",
    "epochs = 6\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"Session Start\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch: \" + str(epoch+1))\n",
    "        train_images, train_labels = shuffle(train_images, train_labels)\n",
    "        for batch in range(1,int(train_images.shape[0]/batch_size)):\n",
    "            optcost = sess.run([optimizer, cost, accuracy], feed_dict={learning_rate: rate, momentum: moment,\n",
    "                                                                       convdropout: conv_dropout,\n",
    "                                                                       data_features: train_images[(batch-1)*batch_size:batch*batch_size],\n",
    "                                                                       data_labels: train_labels[(batch-1)*batch_size:batch*batch_size]})\n",
    "            if batch % 10 == 0:\n",
    "                print(\"Non-Cancer Percentage: \" + str(1-np.sum(np.argmax(train_labels[(batch-1)*batch_size:batch*batch_size],1))/batch_size))\n",
    "                print(\"Running Cost (Batch \" + str(batch) + \"): \" + str(optcost[1]) + \", Acc: \" + str(optcost[2]))\n",
    "        valid_images, valid_labels = shuffle(valid_images, valid_labels)\n",
    "        accost = sess.run([accuracy,cost], feed_dict={convdropout: 1.,\n",
    "                                                      data_features: valid_images[:batch_size//2], \n",
    "                                                      data_labels: valid_labels[:batch_size//2]})\n",
    "        print(\"\\nActual Cancer Percentage: \" + str(np.sum(np.argmax(valid_labels[:batch_size],1))/batch_size))\n",
    "        print(\"Cost: \" + str(accost[1]) + \", Accuracy: \" + str(accost[0]))\n",
    "        print(\"\\n\")\n",
    "        saver.save(sess,save_file)\n",
    "        \n",
    "    acc = sess.run(accuracy, feed_dict={learning_rate: rate, momentum: moment,\\\n",
    "                                   convdropout: 1., \\\n",
    "                                    data_features: valid_images, data_labels: valid_labels})\n",
    "    print(\"Validation Accuracy: \" + str(acc))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "## Results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
