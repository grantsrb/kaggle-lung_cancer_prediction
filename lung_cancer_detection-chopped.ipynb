{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lung Cancer Detection Using CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import libraries for data imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Get Working Dataset\n",
    "\n",
    "Read in previously pickled data. The training and validation sets consist of a shuffled assortment of 4 images for each patient not diagnosed with cancer and 8 images for patients diagnosed with cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def pickle2np(data_dict):\n",
    "    images = chop_data(data_dict['features'], 32)\n",
    "    labels = data_dict['labels']\n",
    "    return images,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chop(image, img_size):\n",
    "    limit = image.shape[0]//img_size\n",
    "    chops = []\n",
    "    for i in range(1,limit+1):\n",
    "        for j in range(1,limit+1):\n",
    "            if (i == 1 and j == 1) or (i == 1 and j == limit) or (i == limit and j == 1) or (i == limit and j == limit): continue\n",
    "            chops.append(image[img_size*(i-1):img_size*i,img_size*(j-1):img_size*j])\n",
    "    return np.array(chops, dtype=np.float32)\n",
    "\n",
    "def chop_data(images, img_size):\n",
    "    chopped_images = []\n",
    "    for i,img in enumerate(images):\n",
    "        chopped_images.append(chop(img, img_size))\n",
    "    return np.array([chop for chop in chopped_images], dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_images = []\n",
    "train_labels = []\n",
    "for i in range(1,8):\n",
    "    path = '../downloads/train_set'+str(i)+'.p'\n",
    "    with open(path, mode='rb') as f:\n",
    "        temp_imgs, temp_labels = pickle2np(pickle.load(f))\n",
    "    train_images.append(temp_imgs)\n",
    "    train_labels.append(temp_labels)\n",
    "del temp_imgs\n",
    "del temp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_images = np.concatenate(train_images,axis=0)\n",
    "train_labels = np.concatenate(train_labels,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "valid_images = []\n",
    "valid_labels = []\n",
    "for i in range(1,3):\n",
    "    path = '../downloads/valid_set'+str(i)+'.p'\n",
    "    with open(path, mode='rb') as f:\n",
    "        temp_imgs, temp_labels = pickle2np(pickle.load(f))\n",
    "    valid_images.append(temp_imgs)\n",
    "    valid_labels.append(temp_labels)\n",
    "del temp_imgs\n",
    "del temp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_images = np.concatenate(valid_images,axis=0)\n",
    "valid_labels = np.concatenate(valid_labels,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test_dicts = []\n",
    "# path = '../downloads/test.p'\n",
    "# with open(path, 'rb') as f:\n",
    "#     test_dicts.append(pickle.load(f))\n",
    "# test_images, test_ids = pickle2np(test_dicts)\n",
    "# del test_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "\n",
    "## Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def center_and_normalize(data, mu, dev):\n",
    "    return (data-mu)/dev \n",
    "\n",
    "process_mu = np.mean(train_images)\n",
    "process_dev = np.std(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, n_labels):\n",
    "    encoded_labels = np.zeros((labels.shape[0], n_labels), dtype=np.float32)\n",
    "    for i,label in enumerate(labels):\n",
    "        encoded_labels[i,int(label)] = 1\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(train_images.shape[0]):\n",
    "    train_images[i:i+1] = center_and_normalize(train_images[i:i+1], process_mu, process_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_images, train_labels = shuffle(train_images, train_labels)\n",
    "valid_images,valid_labels = shuffle(valid_images,valid_labels)\n",
    "valid_images = valid_images[:100]\n",
    "valid_labels = valid_labels[:100]\n",
    "valid_images = center_and_normalize(valid_images, process_mu, process_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape([s for s in train_images.shape]+[1])\n",
    "valid_images = valid_images.reshape([s for s in valid_images.shape]+[1])\n",
    "train_labels = one_hot_encode(train_labels,2)\n",
    "valid_labels = one_hot_encode(valid_labels,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "## Tensorflow Approach\n",
    "\n",
    "#### Parameter Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Chopped Images Parameter Creation\n",
    "\n",
    "chopped_convweight_shapes = [\n",
    "    # 32x32\n",
    "    (5,5,1,36), # 28x28\n",
    "    # Pool: 14x14\n",
    "    (5,5,36,16), # 10x10\n",
    "    # Pool: 5x5\n",
    "]\n",
    "chopped_fcweight_shapes = [\n",
    "    (5*5*16,50),\n",
    "    (50,2),\n",
    "    (2,1)\n",
    "]\n",
    "\n",
    "chopped_convweights = [tf.Variable(tf.truncated_normal(shape=x,mean=0,stddev=0.1),name=\"conv\"+str(x[-1])) for x in chopped_convweight_shapes]\n",
    "chopped_convbiases = [tf.Variable(tf.zeros([x[-1]]),name=\"convbias\"+str(x[-1])) for x in chopped_convweight_shapes]\n",
    "\n",
    "chopped_fcweights = [tf.Variable(tf.truncated_normal(shape=x,mean=0,stddev=0.1),name=\"fc\"+str(x[-1])) for x in chopped_fcweight_shapes]\n",
    "chopped_fcbiases = [tf.Variable(tf.zeros([x[-1]]),name=\"fcbias\"+str(x[-1])) for x in chopped_fcweight_shapes]\n",
    "\n",
    "combine_chops_weight = tf.Variable(tf.truncated_normal(shape=(((512//32)**2)-4, 2), mean=0, stddev=0.1),name=\"combine\")\n",
    "combine_chops_bias = tf.Variable(tf.zeros([2]),name=\"combinebias\")\n",
    "\n",
    "\n",
    "del chopped_convweight_shapes\n",
    "del chopped_fcweight_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "#### Neural Net Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv2d(data, weight, bias, stride=1, padding=\"VALID\"):\n",
    "    activations = tf.nn.bias_add(tf.nn.conv2d(data, weight,strides=[1,stride,stride,1],padding=padding),bias)\n",
    "    return tf.nn.elu(activations)\n",
    "\n",
    "def max_pool(data,k=2):\n",
    "    return tf.nn.max_pool(data,ksize=[1,k,k,1],strides=[1,k,k,1],padding=\"VALID\")\n",
    "\n",
    "def conv_net(data, weights, biases, dropout_prob, strides=[]):\n",
    "    if len(strides) == 0: strides = [1]*len(weights)\n",
    "    logits = data\n",
    "    for i,weight in enumerate(weights):\n",
    "        logits = conv2d(logits, weight, biases[i],stride=strides[i])\n",
    "        logits = max_pool(logits)\n",
    "        logits = tf.nn.dropout(logits, dropout_prob)\n",
    "    return logits\n",
    "\n",
    "def fc_net(data, weights, biases):\n",
    "    logits = data\n",
    "    for i,weight in enumerate(weights):\n",
    "        if i < len(weights)-1:\n",
    "            logits = tf.matmul(logits, weight) + biases[i]\n",
    "            logits = tf.nn.elu(logits)\n",
    "    return tf.matmul(logits,weights[-1])+biases[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_labels = tf.placeholder(tf.float32, [None, 2], name='data_labels')\n",
    "\n",
    "convdropout = tf.placeholder(tf.float32, name=\"convdropout\")\n",
    "momentum = tf.placeholder(tf.float32, name=\"momentum\")\n",
    "learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Chopped Datas\n",
    "\n",
    "batch_size = 90\n",
    "\n",
    "def chopped_net(datas, conv_weights, conv_biases, fc_weights, fc_biases, batch_size, dropout_prob, strides=[]):\n",
    "    combined_logits = []\n",
    "    for i in range(batch_size):\n",
    "        logits = conv_net(datas[i],conv_weights,conv_biases,dropout_prob,strides=strides)\n",
    "        logits = tf.reshape(logits, [252,fc_weights[0].get_shape().as_list()[0]])\n",
    "        logits = tf.nn.dropout(logits,dropout_prob)\n",
    "        logits = fc_net(logits, fc_weights, fc_biases)\n",
    "        combined_logits.append(tf.reshape(logits,[252]))\n",
    "    outputs = tf.stack(combined_logits)\n",
    "    return outputs\n",
    "\n",
    "def combine_chopped_logits(combined_logits, weight, bias):\n",
    "    return tf.matmul(combined_logits, weight) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "chopped_features = tf.placeholder(tf.float32, [None]+[s for s in train_images.shape[1:]], name=\"chopped_features\")\n",
    "\n",
    "chopped_logits = chopped_net(chopped_features, chopped_convweights, chopped_convbiases, chopped_fcweights, chopped_fcbiases, batch_size, convdropout)\n",
    "chopped_logits = combine_chopped_logits(chopped_logits, combine_chops_weight, combine_chops_bias)\n",
    "\n",
    "chopped_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=chopped_logits,labels=data_labels))\n",
    "chopped_optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=momentum).minimize(chopped_cost)\n",
    "\n",
    "chopped_equals_list = tf.equal(tf.argmax(chopped_logits,1), tf.argmax(data_labels,1))\n",
    "chopped_accuracy = tf.reduce_mean(tf.cast(chopped_equals_list,tf.float32))\n",
    "\n",
    "save_file = './net.ckpt'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Session\n",
    "\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Start\n",
      "Epoch: 1\n",
      "Non-Cancer Percentage: 0.522222222222\n",
      "Cost (Batch 10): 0.689976, Acc: 0.544444\n",
      "Non-Cancer Percentage: 0.611111111111\n",
      "Cost (Batch 20): 0.680076, Acc: 0.611111\n",
      "Non-Cancer Percentage: 0.544444444444\n",
      "Cost (Batch 30): 0.698462, Acc: 0.544444\n",
      "Non-Cancer Percentage: 0.511111111111\n",
      "Cost (Batch 40): 0.712991, Acc: 0.511111\n",
      "Non-Cancer Percentage: 0.611111111111\n",
      "Cost (Batch 50): 0.669514, Acc: 0.611111\n",
      "Non-Cancer Percentage: 0.6\n",
      "Cost (Batch 60): 0.680355, Acc: 0.6\n",
      "\n",
      "Actual Cancer Percentage: 0.411111111111\n",
      "Cost: 0.677118, Accuracy: 0.588889\n",
      "\n",
      "\n",
      "Epoch: 2\n",
      "Non-Cancer Percentage: 0.588888888889\n",
      "Cost (Batch 10): 0.671745, Acc: 0.588889\n",
      "Non-Cancer Percentage: 0.544444444444\n",
      "Cost (Batch 20): 0.688255, Acc: 0.544444\n",
      "Non-Cancer Percentage: 0.666666666667\n",
      "Cost (Batch 30): 0.655402, Acc: 0.666667\n",
      "Non-Cancer Percentage: 0.744444444444\n",
      "Cost (Batch 40): 0.613319, Acc: 0.744444\n",
      "Non-Cancer Percentage: 0.588888888889\n",
      "Cost (Batch 50): 0.677838, Acc: 0.588889\n",
      "Non-Cancer Percentage: 0.588888888889\n",
      "Cost (Batch 60): 0.677652, Acc: 0.588889\n",
      "\n",
      "Actual Cancer Percentage: 0.433333333333\n",
      "Cost: 0.685826, Accuracy: 0.566667\n",
      "\n",
      "\n",
      "Epoch: 3\n",
      "Non-Cancer Percentage: 0.644444444444\n",
      "Cost (Batch 10): 0.654339, Acc: 0.644444\n",
      "Non-Cancer Percentage: 0.6\n",
      "Cost (Batch 20): 0.673157, Acc: 0.6\n",
      "Non-Cancer Percentage: 0.544444444444\n",
      "Cost (Batch 30): 0.695063, Acc: 0.544444\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5fb5e0b1f51a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                                           \u001b[0mconvdropout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconv_dropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                           \u001b[0mchopped_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                           data_labels: train_labels[batch*batch_size-batch_size:batch*batch_size]})\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Non-Cancer Percentage: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rate = .05\n",
    "moment = .75\n",
    "conv_dropout = .5\n",
    "fc_dropout = .5\n",
    "epochs = 6\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"Session Start\")\n",
    "    n_batches = int(train_images.shape[0]/batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch: \" + str(epoch+1))\n",
    "        train_images, train_labels = shuffle(train_images, train_labels)\n",
    "        for batch in range(1,n_batches):\n",
    "            optcost = sess.run([chopped_optimizer, chopped_cost, chopped_accuracy], \n",
    "                               feed_dict={learning_rate: rate, momentum: moment,\n",
    "                                          convdropout: conv_dropout,\n",
    "                                          chopped_features: train_images[batch*batch_size-batch_size:batch*batch_size],\n",
    "                                          data_labels: train_labels[batch*batch_size-batch_size:batch*batch_size]})\n",
    "            if batch % 10 == 0:\n",
    "                print(\"Non-Cancer Percentage: \" + str(1-np.sum(np.argmax(train_labels[batch*batch_size-batch_size:batch*batch_size],1))/batch_size))\n",
    "                print(\"Cost (Batch \" + str(batch) + \"): \" + str(optcost[1]) + \", Acc: \" + str(optcost[2]))\n",
    "        valid_images, valid_labels = shuffle(valid_images, valid_labels)\n",
    "        accost = sess.run([chopped_accuracy,chopped_cost], feed_dict={convdropout: 1., \n",
    "                                                                      chopped_features: valid_images[:batch_size], \n",
    "                                                                      data_labels: valid_labels[:batch_size]})\n",
    "        print(\"\\nActual Cancer Percentage: \" + str(np.sum(np.argmax(valid_labels[:batch_size],1))/batch_size))\n",
    "        print(\"Cost: \" + str(accost[1]) + \", Accuracy: \" + str(accost[0]))\n",
    "        print(\"\\n\")\n",
    "        saver.save(sess,save_file)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "## Results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
