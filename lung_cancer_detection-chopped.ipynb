{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lung Cancer Detection Using CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import libraries for data imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "\n",
    "def show_img(image):\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import and Convert Data\n",
    "\n",
    "First I import the images and convert them to numpy arrays for experimentation and development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "external_drive_path = '/Volumes/WhiteElephant/'\n",
    "home_path = os.getcwd()\n",
    "os.chdir(external_drive_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Get Working Dataset\n",
    "\n",
    "Read in previously pickled data. The training and validation sets consist of a shuffled assortment of 4 images for each patient not diagnosed with cancer and 8 images for patients diagnosed with cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def pickle2np(dicts):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for data_dict in dicts:\n",
    "        images.append(data_dict['features'])\n",
    "        labels.append(data_dict['labels'])\n",
    "    return np.concatenate([img for img in images],axis=0), np.concatenate([lbl for lbl in labels], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_dicts = []\n",
    "for i in range(1,8):\n",
    "    path = './aws_pickle_files/train_set'+str(i)+'.p'\n",
    "    with open(path, mode='rb') as f:\n",
    "        train_dicts.append(pickle.load(f))\n",
    "train_images, train_labels = pickle2np(train_dicts)\n",
    "del train_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "valid_dicts = []\n",
    "for i in range(1,3):\n",
    "    path = './aws_pickle_files/valid_set'+str(i)+'.p'\n",
    "    with open(path, mode='rb') as f:\n",
    "        valid_dicts.append(pickle.load(f))\n",
    "valid_images, valid_labels = pickle2np(valid_dicts)\n",
    "del valid_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_dicts = []\n",
    "path = './aws_pickle_files/test.p'\n",
    "with open(path, 'rb') as f:\n",
    "    test_dicts.append(pickle.load(f))\n",
    "test_images, test_ids = pickle2np(test_dicts)\n",
    "del test_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "\n",
    "## Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def center_and_normalize(data, mu, dev):\n",
    "    return (data-mu)/dev \n",
    "\n",
    "process_mu = np.mean(train_images)\n",
    "process_dev = np.std(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, n_labels):\n",
    "    encoded_labels = np.zeros((labels.shape[0], n_labels), dtype=np.float32)\n",
    "    for i,label in enumerate(labels):\n",
    "        encoded_labels[i,int(label)] = 1\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images, train_labels = shuffle(train_images, train_labels)\n",
    "valid_images,valid_labels = shuffle(valid_images,valid_labels)\n",
    "valid_images = valid_images[:100]\n",
    "valid_labels = valid_labels[:100]\n",
    "valid_images = center_and_normalize(valid_images, process_mu, process_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape([train_images.shape[i] for i in range(3)]+[1])\n",
    "valid_images = valid_images.reshape([valid_images.shape[i] for i in range(3)]+[1])\n",
    "train_labels = one_hot_encode(train_labels,2)\n",
    "valid_labels = one_hot_encode(valid_labels,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chop(image, img_size):\n",
    "    limit = image.shape[0]//img_size\n",
    "    chops = []\n",
    "    for i in range(1,limit+1):\n",
    "        for j in range(1,limit+1):\n",
    "            if (i == 1 and j == 1) or (i == 1 and j == limit) or (i == limit and j == 1) or (i == limit and j == limit): continue\n",
    "            chops.append(image[img_size*(i-1):img_size*i,img_size*(j-1):img_size*j])\n",
    "    return np.array(chops, dtype=np.float32)\n",
    "\n",
    "def chop_data(images, img_size):\n",
    "    chopped_images = []\n",
    "    print(\"Start Chopping\")\n",
    "    for i,img in enumerate(images):\n",
    "        chopped_images.append(chop(img, img_size))\n",
    "    print(\"End Chopping\")\n",
    "    return np.array([chop for chop in chopped_images], dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Chopping\n",
      "End Chopping\n",
      "Start Chopping\n",
      "End Chopping\n",
      "(5628, 252, 32, 32, 1)\n",
      "(100, 252, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "train_images = chop_data(train_images, 32)\n",
    "valid_images = chop_data(valid_images, 32)\n",
    "print(train_images.shape)\n",
    "print(valid_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = test_images\n",
    "del test_images\n",
    "valid_images = vtest_images\n",
    "del vtest_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "## Tensorflow Approach\n",
    "\n",
    "#### Parameter Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Chopped Images Parameter Creation\n",
    "\n",
    "mu = 0\n",
    "dev = 0.1\n",
    "\n",
    "chopped_convweight_shapes = [\n",
    "    # 32x32\n",
    "    (5,5,1,64), # 28x28\n",
    "    # Pool: 14x14\n",
    "    (5,5,64,16), # 10x10\n",
    "    # Pool: 5x5\n",
    "]\n",
    "chopped_fcweight_shapes = [\n",
    "    (5*5*16,50),\n",
    "    (50,2),\n",
    "    (2,1)\n",
    "]\n",
    "\n",
    "chopped_convweights = [tf.Variable(tf.truncated_normal(shape=x,mean=mu,stddev=dev),name=\"conv\"+str(x[-1])) for x in chopped_convweight_shapes]\n",
    "chopped_convbiases = [tf.Variable(tf.zeros([x[-1]]),name=\"convbias\"+str(x[-1])) for x in chopped_convweight_shapes]\n",
    "\n",
    "chopped_fcweights = [tf.Variable(tf.truncated_normal(shape=x,mean=mu,stddev=dev),name=\"fc\"+str(x[-1])) for x in chopped_fcweight_shapes]\n",
    "chopped_fcbiases = [tf.Variable(tf.zeros([x[-1]]),name=\"fcbias\"+str(x[-1])) for x in chopped_fcweight_shapes]\n",
    "\n",
    "combine_chops_weight = tf.Variable(tf.truncated_normal(shape=(((512//32)**2)-4, 2), mean=mu, stddev=dev),name=\"combine\")\n",
    "combine_chops_bias = tf.Variable(tf.zeros([2]),name=\"combinebias\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "#### Neural Net Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def conv2d(data, weight, bias, stride=1, padding=\"VALID\"):\n",
    "    activations = tf.nn.bias_add(tf.nn.conv2d(data, weight,strides=[1,stride,stride,1],padding=padding),bias)\n",
    "    return tf.nn.elu(activations)\n",
    "\n",
    "def max_pool(data,k=2):\n",
    "    return tf.nn.max_pool(data,ksize=[1,k,k,1],strides=[1,k,k,1],padding=\"VALID\")\n",
    "\n",
    "def conv_net(data, weights, biases, dropout_prob, strides=[]):\n",
    "    if len(strides) == 0: strides = [1]*len(weights)\n",
    "    logits = data\n",
    "    for i,weight in enumerate(weights):\n",
    "        logits = conv2d(logits, weight, biases[i],stride=strides[i])\n",
    "        logits = max_pool(logits)\n",
    "        logits = tf.nn.dropout(logits, dropout_prob)\n",
    "    return logits\n",
    "\n",
    "def fc_net(data, weights, biases, dropout_prob):\n",
    "    logits = data\n",
    "    for i,weight in enumerate(weights):\n",
    "        if i < len(weights)-1:\n",
    "            logits = tf.matmul(logits, weight) + biases[i]\n",
    "            logits = tf.nn.elu(logits)\n",
    "    #         logits = tf.nn.dropout(logits, dropout_prob)\n",
    "    return tf.matmul(logits,weights[-1])+biases[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_features = tf.placeholder(tf.float32, [None]+[train_images.shape[i] for i in range(1,len(train_images.shape))], name=\"data_features\")\n",
    "data_labels = tf.placeholder(tf.float32, [None, 2], name='data_labels')\n",
    "\n",
    "convdropout = tf.placeholder(tf.float32, name=\"convdropout\")\n",
    "fcdropout = tf.placeholder(tf.float32, name=\"fcdropout\")\n",
    "momentum = tf.placeholder(tf.float32, name=\"momentum\")\n",
    "learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Chopped Datas\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "def chopped_net(datas, conv_weights, conv_biases, fc_weights, fc_biases, batch_size, dropout_prob, strides=[]):\n",
    "    combined_logits = []\n",
    "    for i in range(batch_size):\n",
    "        logits = conv_net(datas[i],conv_weights,conv_biases,dropout_prob,strides=strides)\n",
    "        logits = tf.reshape(logits, [252,fc_weights[0].get_shape().as_list()[0]])\n",
    "        logits = tf.nn.dropout(logits,dropout_prob)\n",
    "        logits = fc_net(logits, fc_weights, fc_biases,dropout_prob)\n",
    "        combined_logits.append(tf.reshape(logits,[252]))\n",
    "    outputs = tf.stack(combined_logits)\n",
    "    return outputs\n",
    "\n",
    "def combine_chopped_logits(combined_logits, weight, bias):\n",
    "    return tf.matmul(combined_logits, weight) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "shapes = [shape for shape in train_images.shape]\n",
    "chopped_features = tf.placeholder(tf.float32, [None]+[shapes[i] for i in range(1,len(shapes))], name=\"chopped_features\")\n",
    "\n",
    "chopped_logits = chopped_net(chopped_features, chopped_convweights, chopped_convbiases, chopped_fcweights, chopped_fcbiases, batch_size, convdropout)\n",
    "combined_logits = combine_chopped_logits(chopped_logits, combine_chops_weight, combine_chops_bias)\n",
    "\n",
    "chopped_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=combined_logits,labels=data_labels))\n",
    "chopped_optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=momentum).minimize(chopped_cost)\n",
    "\n",
    "chopped_equals_list = tf.equal(tf.argmax(combined_logits,1), tf.argmax(data_labels,1))\n",
    "chopped_accuracy = tf.reduce_mean(tf.cast(chopped_equals_list,tf.float32))\n",
    "\n",
    "save_file = './net.ckpt'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Session\n",
    "\n",
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rate = .05\n",
    "moment = .75\n",
    "conv_dropout = .5\n",
    "fc_dropout = .5\n",
    "epochs = 6\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"Session Start\")\n",
    "    n_batches = int(train_images.shape[0]/batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch: \" + str(epoch+1))\n",
    "        train_images, train_labels = shuffle(train_images, train_labels)\n",
    "        for batch in range(1,n_batches):\n",
    "            optcost = sess.run([chopped_optimizer, chopped_cost, chopped_accuracy], \n",
    "                               feed_dict={learning_rate: rate, momentum: moment,\n",
    "                                          convdropout: conv_dropout, fcdropout:fc_dropout,\n",
    "                                          chopped_features: train_images[batch*batch_size-batch_size:batch*batch_size],\n",
    "                                          data_labels: train_labels[batch*batch_size-batch_size:batch*batch_size]})\n",
    "            if batch % 10 == 0:\n",
    "                print(\"Non-Cancer Percentage: \" + str(1-np.sum(np.argmax(train_labels[batch*batch_size-batch_size:batch*batch_size],1))/batch_size))\n",
    "                print(\"Cost (Batch \" + str(batch) + \"): \" + str(optcost[1]) + \", Acc: \" + str(optcost[2]))\n",
    "        valid_images, valid_labels = shuffle(valid_images, valid_labels)\n",
    "        accost = sess.run([chopped_accuracy,chopped_cost], feed_dict={convdropout: 1.,\n",
    "                                                                      fcdropout:1., \n",
    "                                                                      chopped_features: valid_images[:batch_size], \n",
    "                                                                      data_labels: valid_labels[:batch_size]})\n",
    "        print(\"\\nActual Cancer Percentage: \" + str(np.sum(np.argmax(valid_labels[:batch_size],1))/batch_size))\n",
    "        print(\"Cost: \" + str(accost[1]) + \", Accuracy: \" + str(accost[0]))\n",
    "        print(\"\\n\")\n",
    "        saver.save(sess,save_file)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_datasets = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Session Start\\n\")\n",
    "    saver.restore(sess, save_file)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch: \" + str(epoch+1))\n",
    "        for i in range(1,n_datasets+1):\n",
    "            file_path = './chopped_data' + str(i) + '.p'\n",
    "            with open(file_path, 'rb') as f:\n",
    "                dataset_dict = pickle.load(f)\n",
    "            chops = dataset_dict['features']\n",
    "            labels = dataset_dict['labels']\n",
    "            chops,labels = shuffle(chops,labels)\n",
    "            n_batches = int(chops.shape[0]/batch_size)\n",
    "            running_acc = 0\n",
    "            for batch in range(1,n_batches):\n",
    "                batch_x = chops[batch*batch_size-batch_size:batch*batch_size]\n",
    "                batch_y = labels[batch*batch_size-batch_size:batch*batch_size]\n",
    "                feed = {learning_rate: rate, momentum: moment,\\\n",
    "                                               convdropout: 1., fcdropout: 1.,\\\n",
    "                                                chopped_features: batch_x, data_labels: batch_y}\n",
    "                acc = sess.run(chopped_accuracy, feed_dict=feed)\n",
    "                running_acc += acc\n",
    "            print(\"Test Accuracy: \" + str(running_acc/n_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Large Scale Training\n",
    "\n",
    "This training uses the pickled data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "## Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "## Results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
